{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc5d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d1926",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea73baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb284882",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fed6ad",
   "metadata": {},
   "source": [
    "## Retrieval Khusus Mr. TyDi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d356f4d9",
   "metadata": {},
   "source": [
    "Isi title dan text dari split dev dan test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2746e37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\recomp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "mr_tydi = load_dataset(\"castorini/mr-tydi\", \"indonesian\")\n",
    "corpus = load_dataset(\"castorini/mr-tydi-corpus\", \"indonesian\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0619799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLING - HAPUS NANTI\n",
    "mr_tydi['dev'] = mr_tydi['dev'].select(range(5))\n",
    "mr_tydi[\"test\"] = mr_tydi[\"test\"].select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a079a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_dict = {row[\"docid\"]: (row[\"title\"], row[\"text\"]) for row in corpus[\"train\"]}\n",
    "\n",
    "def fill_passage_info(example):\n",
    "    for passage in example[\"positive_passages\"]:\n",
    "        docid = passage[\"docid\"]\n",
    "        if docid in corpus_dict:  # Cek apakah docid ada di corpus\n",
    "            passage[\"title\"], passage[\"text\"] = corpus_dict[docid]\n",
    "    \n",
    "    return example\n",
    "\n",
    "# Terapkan fungsi untuk melengkapi positive_passages di split 'dev' dan 'test'\n",
    "mr_tydi[\"dev\"] = mr_tydi[\"dev\"].map(fill_passage_info)\n",
    "mr_tydi[\"test\"] = mr_tydi[\"test\"].map(fill_passage_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b0927e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLING - HAPUS NANTI\n",
    "corpus['train'] = corpus['train'].shuffle(seed=42).select(range(500))\n",
    "corpus_dict = {row[\"docid\"]: (row[\"title\"], row[\"text\"]) for row in corpus['train']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9323a6e2",
   "metadata": {},
   "source": [
    "Ambil 2 passage untuk negative_passages khusus split dev dan test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2bad11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Corpus: 100%|██████████| 4/4 [00:02<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from all_utils import average_pool\n",
    "\n",
    "\n",
    "model_name = \"intfloat/multilingual-e5-small\"\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "embedding_model = AutoModel.from_pretrained(model_name).to(\"cuda:0\")\n",
    "\n",
    "os.makedirs(\"./output\", exist_ok=True)\n",
    "\n",
    "# Buat dictionary {docid: (title, text)} untuk lookup cepat dari corpus\n",
    "corpus_dict = {row[\"docid\"]: (row[\"title\"], row[\"text\"]) for row in corpus[\"train\"]}\n",
    "\n",
    "# Ambil semua dokumen text dari corpus untuk dijadikan embedding\n",
    "corpus_docids = list(corpus_dict.keys())\n",
    "corpus_texts = [f\"passage: {corpus_dict[docid][0]} | {corpus_dict[docid][1]}\" for docid in corpus_docids]\n",
    "\n",
    "batch_size = 128  # Sesuaikan dengan VRAM yang tersedia\n",
    "corpus_embeddings = []\n",
    "\n",
    "for start_idx in tqdm(range(0, len(corpus_texts), batch_size), desc=\"Encoding Corpus\"):\n",
    "    end_idx = min(start_idx + batch_size, len(corpus_texts))\n",
    "    batch_texts = corpus_texts[start_idx:end_idx]\n",
    "\n",
    "    batch_dict = embedding_tokenizer(batch_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "    batch_dict = {k: v.to(\"cuda:0\") for k, v in batch_dict.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**batch_dict)\n",
    "\n",
    "    embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "    embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "    corpus_embeddings.append(embeddings.to(torch.float32).cpu())\n",
    "\n",
    "corpus_embeddings = torch.cat(corpus_embeddings, dim=0).numpy().astype(np.float32)  # Konversi ke NumPy\n",
    "\n",
    "# Buat FAISS index untuk pencarian similarity\n",
    "index = faiss.IndexFlatIP(corpus_embeddings.shape[1])  # IP = Inner Product (Cosine Similarity)\n",
    "index.add(corpus_embeddings)  # Tambahkan corpus embeddings ke FAISS\n",
    "\n",
    "# # Simpan index faiss\n",
    "# faiss.write_index(index, \"../generated_data/raw/faiss_index.idx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db60d7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from preprocess_utils import add_negative_passages\n",
    "add_neg_psgs = partial(add_negative_passages, embedding_tokenizer=embedding_tokenizer, embedding_model=embedding_model, index=index, corpus_docids=corpus_docids, corpus_dict=corpus_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb6dbcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_tydi[\"dev\"] = mr_tydi[\"dev\"].map(add_neg_psgs, with_indices=True, batched=True, batch_size=16)\n",
    "mr_tydi[\"test\"] = mr_tydi[\"test\"].map(add_neg_psgs, with_indices=True, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc018ae1",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1841fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset\n",
    "from datasets import DatasetDict\n",
    "import re\n",
    "\n",
    "# Drop kolom\n",
    "tydiqa_gold = load_dataset(\"khalidalt/tydiqa-goldp\", 'indonesian', trust_remote_code=True)\n",
    "\n",
    "# Rename kolom\n",
    "tydiqa_gold = tydiqa_gold.remove_columns([\"language\", \"document_title\", \"passage_text\"])\n",
    "tydiqa_gold = tydiqa_gold.rename_column(\"id\", \"tydiqa_id\")\n",
    "tydiqa_gold = tydiqa_gold.rename_column(\"question_text\", \"query\")\n",
    "\n",
    "# Restrukturisasi kolom\n",
    "def extract_text(example):\n",
    "    example[\"answers\"] = example[\"answers\"][\"text\"] \n",
    "    return example\n",
    "\n",
    "tydiqa_gold = DatasetDict({\n",
    "    split: dataset.map(extract_text)\n",
    "    for split, dataset in tydiqa_gold.items()\n",
    "})\n",
    "\n",
    "# Fungsi untuk membersihkan teks: hapus newline & whitespace berlebih\n",
    "def clean_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "# Fungsi untuk membersihkan dan memilih jawaban terpendek\n",
    "def clean_tydiqa(example):\n",
    "    # Bersihkan query\n",
    "    example[\"query\"] = clean_text(example[\"query\"])\n",
    "    \n",
    "    # Bersihkan answers dan pilih jawaban terpendek jika ada lebih dari satu\n",
    "    cleaned_answers = [clean_text(ans) for ans in example[\"answers\"]]\n",
    "    example[\"answers\"] = min(cleaned_answers, key=len) if cleaned_answers else \"\"  # Pilih jawaban terpendek\n",
    "\n",
    "    return example\n",
    "\n",
    "def clean_mr_tydi(example):\n",
    "    example[\"query\"] = clean_text(example[\"query\"])\n",
    "    return example\n",
    "\n",
    "# Terapkan pembersihan pada dataset\n",
    "tydiqa_gold_cleaned = DatasetDict({\n",
    "    split: dataset.map(clean_tydiqa)\n",
    "    for split, dataset in tydiqa_gold.items()\n",
    "})\n",
    "\n",
    "mr_tydi_cleaned = DatasetDict({\n",
    "    split: dataset.map(clean_mr_tydi)\n",
    "    for split, dataset in mr_tydi.items()\n",
    "})\n",
    "\n",
    "for split in tydiqa_gold_cleaned.keys():\n",
    "    tydiqa_gold_cleaned[split] = tydiqa_gold_cleaned[split].rename_column(\"answers\", \"answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af053f",
   "metadata": {},
   "source": [
    "## Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bc39fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset berhasil digabungkan berdasarkan `query` dengan struktur mengikuti `mr_tydi_cleaned`.\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, concatenate_datasets\n",
    "\n",
    "tydiqa_gold_combined = concatenate_datasets([tydiqa_gold_cleaned[\"train\"], tydiqa_gold_cleaned[\"validation\"]])\n",
    "\n",
    "# Buat struktur baru mengikuti split dari mr_tydi_cleaned\n",
    "joined_datasets = {}\n",
    "\n",
    "for split, mr_tydi_split in mr_tydi_cleaned.items():\n",
    "    # Buat dictionary {query: row} dari tydiqa_gold_cleaned untuk lookup cepat\n",
    "    tydiqa_gold_dict = {row[\"query\"]: row for row in tydiqa_gold_combined}\n",
    "    \n",
    "    # Buat daftar baru dengan menggabungkan informasi dari mr_tydi_cleaned dan tydiqa_gold_cleaned\n",
    "    new_split_data = []\n",
    "    \n",
    "    for row in mr_tydi_split:\n",
    "        query = row[\"query\"]\n",
    "        tydiqa_data = tydiqa_gold_dict.get(query, None)  # Ambil data dari tydiqa_gold jika ada\n",
    "        \n",
    "        # Gabungkan data (jika tidak ada di tydiqa_gold, biarkan bagian tersebut kosong)\n",
    "        merged_row = {\n",
    "            **row,  # Data dari mr_tydi_cleaned\n",
    "            \"tydiqa_id\": tydiqa_data[\"tydiqa_id\"] if tydiqa_data else None,\n",
    "            \"answer\": tydiqa_data[\"answer\"] if tydiqa_data else None\n",
    "        }\n",
    "        \n",
    "        new_split_data.append(merged_row)\n",
    "\n",
    "    joined_datasets[split] = mr_tydi_split.from_list(new_split_data)\n",
    "\n",
    "merged_dataset = DatasetDict(joined_datasets)\n",
    "\n",
    "print(\"✅ Dataset berhasil digabungkan berdasarkan `query` dengan struktur mengikuti `mr_tydi_cleaned`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e542a9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 4902/4902 [00:02<00:00, 2440.14 examples/s]\n",
      "Filter: 100%|██████████| 5/5 [00:00<00:00, 821.09 examples/s]\n",
      "Filter: 100%|██████████| 5/5 [00:00<00:00, 952.99 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Semua row dengan 'answers = None' telah dihapus dari dataset baru `merged_dataset`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "def remove_none_answers(dataset):\n",
    "    return dataset.filter(lambda row: row[\"answer\"] is not None)\n",
    "\n",
    "merged_dataset = DatasetDict({\n",
    "    \"train\": remove_none_answers(merged_dataset[\"train\"]),\n",
    "    \"dev\": remove_none_answers(merged_dataset[\"dev\"]),\n",
    "    \"test\": remove_none_answers(merged_dataset[\"test\"])\n",
    "})\n",
    "\n",
    "print(\"✅ Semua row dengan 'answers = None' telah dihapus dari dataset baru `merged_dataset`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eaa41c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLING - HAPUS NANTI\n",
    "merged_dataset['train'] = merged_dataset['train'].select(range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ff5b6",
   "metadata": {},
   "source": [
    "## Ambil Top 2 Negative Passages pada Split Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac40159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:02<00:00,  1.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from preprocess_utils import select_top2_negative_passages\n",
    "select_top2_neg_psgs = partial(select_top2_negative_passages, embedding_tokenizer=embedding_tokenizer, embedding_model = embedding_model)\n",
    "merged_dataset[\"train\"] = merged_dataset[\"train\"].map(select_top2_neg_psgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c7b5ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 408.09 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 291.96 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 289.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from preprocess_utils import create_top_3_passages\n",
    "\n",
    "# Terapkan fungsi ke split train, dev, dan test\n",
    "merged_dataset[\"train\"] = merged_dataset[\"train\"].map(create_top_3_passages)\n",
    "merged_dataset[\"dev\"] = merged_dataset[\"dev\"].map(create_top_3_passages)\n",
    "merged_dataset[\"test\"] = merged_dataset[\"test\"].map(create_top_3_passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ef087f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 465.14 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 566.23 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 297.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Memformat passage agar mengandung string \"Judul:...\\nTeks:...\"\"\n",
    "def format_passages(example, psgs_col='top_3_passages',  title_col='title', text_col='text'):\n",
    "    psgs = example[psgs_col]\n",
    "    formatted_psgs = []\n",
    "    for psg in psgs:\n",
    "        formatted_psgs.append(f\"{psg[title_col]} | {psg[text_col]}\")\n",
    "\n",
    "    example['passages'] = formatted_psgs\n",
    "\n",
    "    return example\n",
    "\n",
    "for split in merged_dataset.keys():\n",
    "    merged_dataset[split] = merged_dataset[split].map(format_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1847d964",
   "metadata": {},
   "source": [
    "## Truncate passages agar panjangnya seragam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7081cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 174.24 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 146.17 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 137.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "t5_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "def truncate_passages(examples):\n",
    "    # Tokenisasi setiap passage dalam kolom 'passages' dan batasi panjangnya menjadi 512 token\n",
    "    truncated_passages = []\n",
    "    for passage in examples['passages']:\n",
    "        # Tokenize each passage and truncate it to 512 tokens\n",
    "        tokenized = t5_tokenizer(passage, padding='max_length', truncation=True, max_length=512, add_special_tokens=False)\n",
    "        \n",
    "        # Decode input_ids menjadi string dan tambahkan ke list truncated_passages\n",
    "        truncated_passages.append(t5_tokenizer.decode(tokenized['input_ids'], skip_special_tokens=True))\n",
    "    \n",
    "    examples['trunc_passages'] = truncated_passages\n",
    "    return examples\n",
    "\n",
    "# Terapkan fungsi ke dataset\n",
    "for split in merged_dataset.keys():\n",
    "    merged_dataset[split] = merged_dataset[split].map(truncate_passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eff8e06",
   "metadata": {},
   "source": [
    "## Mengurutkan dan Melabeli Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e54493bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing ranked_truncPassages_with_labels: 100%|██████████| 5/5 [00:00<00:00, 33.94it/s]\n",
      "Processing ranked_truncPassages_with_labels: 100%|██████████| 5/5 [00:00<00:00, 45.82it/s]\n",
      "Processing ranked_truncPassages_with_labels: 100%|██████████| 4/4 [00:00<00:00, 47.47it/s]\n"
     ]
    }
   ],
   "source": [
    "from all_utils import apply_similarity_ranking_to_dataset\n",
    "# Me-rangking passages berdasarkan skor similarity\n",
    "for split in merged_dataset.keys():\n",
    "    merged_dataset[split] = apply_similarity_ranking_to_dataset(\n",
    "        merged_dataset[split], \n",
    "        text_col=\"trunc_passages\",\n",
    "        output_col=\"ranked_truncPassages_with_labels\", \n",
    "        tokenizer=embedding_tokenizer, \n",
    "        model=embedding_model, \n",
    "        device = embedding_model.device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd725a",
   "metadata": {},
   "source": [
    "## Distribusi ulang split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd64e577",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Tentukan jumlah baris yang ingin dipindahkan\n",
    "num_rows_to_move = 578\n",
    "\n",
    "# Pilih 577 baris acak dari split 'dev'\n",
    "dev_dataset = merged_dataset['dev']\n",
    "\n",
    "selected_rows = dev_dataset.select(range(num_rows_to_move))  # Ambil 578 baris pertama setelah shuffle\n",
    "\n",
    "# Hapus 577 baris yang sudah dipilih dari 'dev'\n",
    "remaining_dev = dev_dataset.select(range(num_rows_to_move, len(dev_dataset)))\n",
    "\n",
    "# Konversi ke DataFrame pandas untuk dapat menggunakan concat\n",
    "train_df = merged_dataset['train'].to_pandas()\n",
    "selected_rows_df = selected_rows.to_pandas()\n",
    "\n",
    "# Gabungkan keduanya dengan pandas.concat\n",
    "new_train_df = pd.concat([train_df, selected_rows_df], ignore_index=True)\n",
    "\n",
    "# Kembali ke dataset HuggingFace dari DataFrame\n",
    "new_train = Dataset.from_pandas(new_train_df)\n",
    "\n",
    "# Perbarui split train dan dev\n",
    "merged_dataset['train'] = new_train\n",
    "merged_dataset['dev'] = remaining_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6806ef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 293.00 examples/s]\n",
      "Map: 100%|██████████| 5/5 [00:00<00:00, 385.87 examples/s]\n",
      "Map: 100%|██████████| 4/4 [00:00<00:00, 246.00 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': ['query_id', 'query', 'positive_passages', 'negative_passages', 'tydiqa_id', 'answer', 'top_3_passages', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels', 'sorted_truncPassages'], 'dev': ['query_id', 'query', 'positive_passages', 'negative_passages', 'tydiqa_id', 'answer', 'top_3_passages', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels', 'sorted_truncPassages'], 'test': ['query_id', 'query', 'positive_passages', 'negative_passages', 'tydiqa_id', 'answer', 'top_3_passages', 'passages', 'trunc_passages', 'ranked_truncPassages_with_labels', 'sorted_truncPassages']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def extract_sorted_passages(row):\n",
    "    passages = row['ranked_truncPassages_with_labels']\n",
    "    sorted_texts = [passage['text'] for passage in passages]\n",
    "    return sorted_texts\n",
    "\n",
    "final_dataset = merged_dataset.map(lambda row: {'sorted_truncPassages': extract_sorted_passages(row)}, batched=False)\n",
    "\n",
    "print(final_dataset.column_names)\n",
    "final_dataset = final_dataset.rename_column('answer', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f064977b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 5/5 [00:00<00:00, 238.85 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5/5 [00:00<00:00, 370.10 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4/4 [00:00<00:00, 278.99 examples/s]\n"
     ]
    }
   ],
   "source": [
    "final_dataset.save_to_disk(\"./output/raw_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce786b5",
   "metadata": {},
   "source": [
    "# Memperbaiki Label dari Split Test\n",
    "Labelnya bisa beragam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7796b050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 1) Load split validation\n",
    "ds = load_dataset(\"khalidalt/tydiqa-goldp\", \"indonesian\", split=\"validation\")\n",
    "\n",
    "def dedup_keep_order(seq):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for x in seq:\n",
    "        if x not in seen:\n",
    "            seen.add(x)\n",
    "            result.append(x)\n",
    "    return result\n",
    "\n",
    "def select_answers(example):\n",
    "    # ambil answers.text (maks 3 item di dataset ini), simpan versi asli (strip, kapital dipertahankan)\n",
    "    orig = [t.strip() for t in example[\"answers\"][\"text\"]]\n",
    "    if not orig:  # jaga-jaga\n",
    "        example[\"selected_answer\"] = []\n",
    "        return example\n",
    "\n",
    "    # versi normalisasi untuk pengecekan substring\n",
    "    norm = [t.lower().strip() for t in orig]\n",
    "\n",
    "    # urutkan indeks berdasarkan panjang normalized (terpendek -> terpanjang)\n",
    "    order = sorted(range(len(norm)), key=lambda i: len(norm[i]))\n",
    "    # terpendek\n",
    "    s1_idx = order[0]\n",
    "    s1_norm, s1_orig = norm[s1_idx], orig[s1_idx]\n",
    "\n",
    "    # --- aturan awal: jika s1 ada di semua elemen lain -> hanya s1\n",
    "    contained_in_all = all(\n",
    "        (s1_norm in nt) for i, nt in enumerate(norm) if i != s1_idx\n",
    "    )\n",
    "    if contained_in_all:\n",
    "        selected = [s1_orig]\n",
    "    else:\n",
    "        selected = None\n",
    "        if len(norm) >= 2:\n",
    "            s2_idx = order[1]\n",
    "            s2_norm, s2_orig = norm[s2_idx], orig[s2_idx]\n",
    "\n",
    "            s2_not_superset = (s1_norm not in s2_norm)\n",
    "\n",
    "            if s2_not_superset:\n",
    "                exists_longer_that_contains_s2 = any(\n",
    "                    (i != s2_idx) and (len(nt) > len(s2_norm)) and (s2_norm in nt)\n",
    "                    for i, nt in enumerate(norm)\n",
    "                )\n",
    "                if exists_longer_that_contains_s2:\n",
    "                    selected = [s1_orig, s2_orig]\n",
    "                else:\n",
    "                    selected = orig[:]\n",
    "\n",
    "        if selected is None:\n",
    "            others = [o for o, nt in zip(orig, norm) if nt is not s1_norm and (s1_norm not in nt)]\n",
    "            selected = [s1_orig] + others\n",
    "\n",
    "    # hapus duplikat dengan menjaga urutan\n",
    "    example[\"selected_answer\"] = dedup_keep_order(selected)\n",
    "    return example\n",
    "\n",
    "# 2) Tambahkan kolom selected_answer\n",
    "ds = ds.map(select_answers)\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "# 3) Load file JSON berisi pembaruan\n",
    "json_path = Path(\"labels_to_be_updated.json\")\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    updates = json.load(f)\n",
    "\n",
    "# Ubah jadi dict agar lebih cepat diakses\n",
    "update_map = {item[\"query\"]: item[\"new_answer\"] for item in updates if \"query\" in item}\n",
    "\n",
    "import re\n",
    "def apply_manual_updates(example):\n",
    "    # Ambil teks pertanyaan dan hilangkan spasi berlebih (di awal, akhir, dan tengah)\n",
    "    q = example[\"question_text\"]\n",
    "    q = re.sub(r\"\\s+\", \" \", q.strip())  # ganti semua whitespace berturut jadi satu spasi\n",
    "\n",
    "    # Lakukan hal yang sama juga untuk key di update_map agar konsisten\n",
    "    normalized_update_map = {re.sub(r\"\\s+\", \" \", k.strip()): v for k, v in update_map.items()}\n",
    "\n",
    "    if q in normalized_update_map:\n",
    "        example[\"selected_answer\"] = normalized_update_map[q]\n",
    "    return example\n",
    "\n",
    "# 5) Terapkan ke dataset\n",
    "ds = ds.map(apply_manual_updates)\n",
    "\n",
    "# 6) (Opsional) Simpan dataset hasil update\n",
    "# ds.save_to_disk(\"tydiqa_indonesian_updated\")\n",
    "\n",
    "# 7) Contoh cek\n",
    "print(ds)\n",
    "print(ds[0][\"question_text\"])\n",
    "print(ds[0][\"selected_answer\"])\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# dataset test_raw\n",
    "test_raw = load_dataset(\"khalidrizki/postretrieve-raw-dataset-v2\", split=\"test\")\n",
    "\n",
    "# buat dictionary mapping id -> selected_answer\n",
    "id2selected = {row[\"id\"]: row[\"selected_answer\"] for row in ds}\n",
    "id2selected\n",
    "\n",
    "def add_label_list(example):\n",
    "    tid = example[\"tydiqa_id\"]\n",
    "    if tid in id2selected:\n",
    "        example[\"label_list\"] = id2selected[tid]\n",
    "    else:\n",
    "        example[\"label_list\"] = [example[\"label\"].strip()]\n",
    "    return example\n",
    "\n",
    "# tambahkan kolom baru\n",
    "test_raw = test_raw.map(add_label_list)\n",
    "test_raw = test_raw.remove_columns([\"label\"])\n",
    "# cek hasil\n",
    "print(test_raw)\n",
    "print(test_raw[0][\"tydiqa_id\"], test_raw[0][\"label_list\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recomp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
